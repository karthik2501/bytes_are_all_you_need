{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ffc23b09-4c8d-46ec-a404-ab4054de9798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2df443a4-d810-40ef-bf1d-fe6415434ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "        position = torch.arange(0, max_seq_length).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pos_encoding = torch.zeros(max_seq_length, d_model)\n",
    "        pos_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        pos_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        pos_encoding = pos_encoding.unsqueeze(0)\n",
    "\n",
    "        self.register_buffer('pos_encoding', pos_encoding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pos_encoding[:, :x.size(1)].clone().detach()\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0997b63f-794c-4d81-9e83-57232b83ac91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ByteFormer(nn.Module):\n",
    "    def __init__(self,d_model,nhead,num_layers,embedding_dim,kernel_size,stride,num_classes,max_seq_length):\n",
    "        super(ByteFormer, self).__init__()\n",
    "        self.embedding_layer = nn.Embedding(256,embedding_dim)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "        self.conv = nn.Conv1d(embedding_dim,embedding_dim,kernel_size,stride)\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.classifier = nn.Linear(embedding_dim,num_classes)\n",
    "        self.output = nn.Softmax(dim=0)\n",
    "    def forward(self,x):\n",
    "        x = self.embedding_layer(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = torch.transpose(x,1,2)\n",
    "        x = torch.transpose(self.conv(x),2,1)\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            x = encoder_layer(x)\n",
    "        n = x.shape[1]\n",
    "        x = torch.sum(x,dim=1)/n\n",
    "        x = self.classifier(x)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a9c544-2d3d-44cd-9994-31a430d63df0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "00be4bdf-fae2-4161-b459-fe06322ae663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d94c057c-0197-476d-b805-db4736484e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = '/home/notebook/data/group/cifar-10-batches-py/data_batch_1'\n",
    "path2 = '/home/notebook/data/group/cifar-10-batches-py/data_batch_2'\n",
    "path3 = '/home/notebook/data/group/cifar-10-batches-py/data_batch_3'\n",
    "path4 = '/home/notebook/data/group/cifar-10-batches-py/data_batch_4'\n",
    "path5 = '/home/notebook/data/group/cifar-10-batches-py/data_batch_5'\n",
    "path_test = '/home/notebook/data/group/cifar-10-batches-py/test_batch'\n",
    "path_d = '/home/notebook/data/group/cifar-10-batches-py/batches.meta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "444547ce-f285-4c73-952e-eae4c930fc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_batch_1 = unpickle(path1)\n",
    "data_batch_2 = unpickle(path2)\n",
    "data_batch_3 = unpickle(path3)\n",
    "data_batch_4 = unpickle(path4)\n",
    "data_batch_5 = unpickle(path5)\n",
    "test_batch = unpickle(path_test)\n",
    "batches_meta = unpickle(path_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "556acbc3-8827-4af4-aac9-dc5d63845884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([b'batch_label', b'labels', b'data', b'filenames'])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_batch_1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6d298eff-7fb1-492e-a4e9-f51f5bad72cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([b'batch_label', b'labels', b'data', b'filenames'])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "57ac74ea-c052-4478-a180-9924d19e44bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([b'num_cases_per_batch', b'label_names', b'num_vis'])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches_meta.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "541462ed-d312-434b-a3db-4247db2420db",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data1,training_labels1 = torch.tensor(data_batch_1[b'data']).to(torch.int),data_batch_1[b'labels']\n",
    "training_data2,training_labels2 = torch.tensor(data_batch_2[b'data']).to(torch.int),data_batch_2[b'labels']\n",
    "training_data3,training_labels3 = torch.tensor(data_batch_3[b'data']).to(torch.int),data_batch_3[b'labels']\n",
    "training_data4,training_labels4 = torch.tensor(data_batch_4[b'data']).to(torch.int),data_batch_4[b'labels']\n",
    "training_data5,training_labels5 = torch.tensor(data_batch_5[b'data']).to(torch.int),data_batch_5[b'labels']\n",
    "test_data,test_labels = torch.tensor(test_batch[b'data']).to(torch.int),test_batch[b'labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "25b1da37-700e-44ca-befd-beff121ab37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5a8f365b-79a7-4196-bd79-79196d14ca55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3072\n",
      "100\n",
      "Epoch [1/5] - Train Loss: 2.3025 - Train Accuracy: 5.00% - Test Loss: 2.3024 - Test Accuracy: 11.00%\n",
      "Epoch [2/5] - Train Loss: 2.3012 - Train Accuracy: 52.00% - Test Loss: 2.3019 - Test Accuracy: 12.00%\n",
      "Epoch [3/5] - Train Loss: 2.2994 - Train Accuracy: 71.00% - Test Loss: 2.3008 - Test Accuracy: 13.00%\n",
      "Epoch [4/5] - Train Loss: 2.2961 - Train Accuracy: 84.00% - Test Loss: 2.2973 - Test Accuracy: 16.00%\n",
      "Epoch [5/5] - Train Loss: 2.2953 - Train Accuracy: 78.00% - Test Loss: 2.2924 - Test Accuracy: 14.00%\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.00008\n",
    "num_epochs = 5\n",
    "\n",
    "# Convert training and test data to tensors\n",
    "k = 100\n",
    "training_data = training_data1[:k]\n",
    "test_data = test_data[:k]\n",
    "training_labels = torch.LongTensor(training_labels1[:k])\n",
    "test_labels = torch.LongTensor(test_labels[:k])\n",
    "\n",
    "train_dataset = TensorDataset(training_data.to(device), training_labels.to(device))\n",
    "test_dataset = TensorDataset(test_data.to(device), test_labels.to(device))\n",
    "\n",
    "\n",
    "\n",
    "model = ByteFormer(192,3,9,192,32,16,10,3072)\n",
    "# Define the loss function and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Mini-batch training loop\n",
    "    for inputs,labels in train_loader:\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track training loss and accuracy\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate average training loss and accuracy\n",
    "    train_loss /= (len(train_loader))\n",
    "    train_accuracy = 100 * correct / total\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Evaluation on the test set\n",
    "        for inputs,labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate average test loss and accuracy\n",
    "    test_loss /= (len(test_loader))\n",
    "    test_accuracy = 100 * correct / total\n",
    "\n",
    "    # Print training progress\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {train_loss:.4f} - Train Accuracy: {train_accuracy:.2f}% - \"\n",
    "          f\"Test Loss: {test_loss:.4f} - Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "# Save the trained model\n",
    "# torch.save(model.state_dict(), 'model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8516341a-d342-4028-9ab5-9d6af201aa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "files = os.listdir(\"./mypart2\")\n",
    "files2 = os.listdir(\"./gt2\")\n",
    "for file in files:\n",
    "    if file not in files2:\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12f4c6e-a82a-4d25-a430-6b7a0e582ce7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
